{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f9a687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ 크롤링 시작: 선크림_선스틱\n",
      "[1/2] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000150624\n",
      "[2/2] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000202346\n",
      "완료: 선크림_선스틱 | 상품 2개, 리뷰 10개\n",
      "\n",
      "▶ 크롤링 시작: 선크림_선스프레이-선패치\n",
      "[1/2] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000175125\n",
      "[2/2] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000224981\n",
      "완료: 선크림_선스프레이-선패치 | 상품 2개, 리뷰 10개\n",
      "\n",
      "▶ 크롤링 시작: 선크림_선쿠션\n",
      "[1/2] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000167182\n",
      "[2/2] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000188223\n",
      "완료: 선크림_선쿠션 | 상품 2개, 리뷰 5개\n",
      "\n",
      "▶ 크롤링 시작: 선크림_태닝-애프터선\n",
      "[1/2] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000198631\n",
      "[2/2] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000168093\n",
      "완료: 선크림_태닝-애프터선 | 상품 2개, 리뷰 10개\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# [0] 드라이버 세팅\n",
    "def setup_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "# [1] 탭 클릭\n",
    "def click_tab(driver, tab_text):\n",
    "    try:\n",
    "        a = driver.find_element(By.XPATH, f\"//ul[contains(@class,'prd_detail_tab')]//a[text()='{tab_text}']\")\n",
    "        a.click()\n",
    "        time.sleep(1)\n",
    "        return True\n",
    "    except:\n",
    "        for a in driver.find_elements(By.CSS_SELECTOR, \".prd_detail_tab a\"):\n",
    "            if tab_text in a.text:\n",
    "                a.click()\n",
    "                time.sleep(1)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# [2] 미디어 파싱\n",
    "def parse_product_media(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    media_urls = []\n",
    "    for tag in soup.select(\"div.detail_area *\"):\n",
    "        src = tag.get(\"src\") or tag.get(\"srcset\")\n",
    "        if src and src.startswith(\"https://\"):\n",
    "            media_urls.append(src)\n",
    "    return media_urls\n",
    "\n",
    "# [3] 제품 상세 파싱\n",
    "def parse_product_detail(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    name = soup.select_one(\"p.prd_name\")\n",
    "    brand = soup.select_one(\"p.prd_brand a\")\n",
    "    old_price = soup.select_one(\"span.price-1 strike\")\n",
    "    sale_price = soup.select_one(\"span.price-2 strong\")\n",
    "    img = soup.select_one(\"#mainImg\")\n",
    "    ing, origin = \"\", \"\"\n",
    "\n",
    "    artc = soup.find(\"div\", id=\"artcInfo\")\n",
    "    if artc:\n",
    "        for dl in artc.select(\"dl.detail_info_list\"):\n",
    "            dt, dd = dl.find(\"dt\"), dl.find(\"dd\")\n",
    "            if dt and dd:\n",
    "                title = dt.get_text(strip=True)\n",
    "                value = dd.get_text(\" \", strip=True)\n",
    "                if \"모든 성분\" in title:\n",
    "                    ing = value\n",
    "                elif \"제조국\" in title:\n",
    "                    origin = value\n",
    "\n",
    "    options = []\n",
    "    for li in soup.select(\"ul.sel_option_list > li[optgoodsinfo]\"):\n",
    "        txt = li.select_one(\"span.txt\").get_text(strip=True)\n",
    "        code, no = li[\"optgoodsinfo\"].split(\":\")\n",
    "        lgc = li.find(\"input\", {\"name\": \"gdasLgcGoodsNo\"})[\"value\"]\n",
    "        options.append({\"옵션명\": txt, \"상품코드\": code, \"아이템번호\": no, \"lgcGoodsNo\": lgc})\n",
    "\n",
    "    media_list = parse_product_media(html)\n",
    "\n",
    "    return {\n",
    "        \"상품명\": name.get_text(strip=True) if name else \"\",\n",
    "        \"브랜드\": brand.get_text(strip=True) if brand else \"\",\n",
    "        \"정가\": old_price.get_text(strip=True) if old_price else \"\",\n",
    "        \"할인가\": sale_price.get_text(strip=True) if sale_price else \"\",\n",
    "        \"이미지\": img[\"src\"] if img else \"\",\n",
    "        \"성분\": ing, \"제조국\": origin,\n",
    "        \"옵션개수\": len(options),\n",
    "        \"옵션리스트\": options,\n",
    "        \"상세미디어목록\": media_list\n",
    "    }\n",
    "\n",
    "# [4] 리뷰 파싱\n",
    "def parse_reviews(html, max_count=10):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    reviews = []\n",
    "    poll_data = {}\n",
    "\n",
    "    ul = soup.find(\"ul\", id=\"gdasList\")\n",
    "    if not ul:\n",
    "        return reviews\n",
    "\n",
    "    for li in ul.find_all(\"li\", recursive=False)[:max_count]:\n",
    "        reviewer = li.select_one(\"p.info_user a.id\")\n",
    "        rating = li.select_one(\"span.point\")\n",
    "        score_span = li.select_one(\"div.score_area span.point\")\n",
    "        date = li.select_one(\"span.date\")\n",
    "        opt = li.select_one(\"p.item_option\")\n",
    "        txt = li.select_one(\"div.txt_inner\")\n",
    "        rec = li.select_one(\".recom_area span.num\")\n",
    "\n",
    "        review_data = {\n",
    "            \"리뷰어\": reviewer.get_text(strip=True) if reviewer else \"\",\n",
    "            \"평점\": re.search(r\"([\\d\\.]+)점\", rating.get_text(strip=True)).group(1) if rating else \"\",\n",
    "            \"최대평점\": re.search(r\"(\\d+)점만점에\", score_span.get_text(strip=True)).group(1) if score_span else \"\",\n",
    "            \"날짜\": date.get_text(strip=True) if date else \"\",\n",
    "            \"옵션\": opt.get_text(strip=True).replace(\"[옵션]\", \"\") if opt else \"\",\n",
    "            \"본문\": txt.get_text(\" \", strip=True) if txt else \"\",\n",
    "            \"추천수\": rec.get_text(strip=True) if rec else \"0\",\n",
    "            \"태그\": [s.get_text(strip=True) for s in li.select(\".review_tag span\")],\n",
    "            \"사용자 피부 정보\": [s.get_text(strip=True) for s in li.select(\"p.tag span\")]\n",
    "        }\n",
    "\n",
    "        # [설문 블록 정보 추출] (예: 발색력 아주 만족해요 등)\n",
    "        poll_dl_tags = li.select(\"div.poll_sample dl.poll_type1\")\n",
    "        for dl in poll_dl_tags:\n",
    "            dt_tag = dl.select_one(\"dt span\")\n",
    "            dd_tag = dl.select_one(\"dd span\")\n",
    "            if dt_tag and dd_tag:\n",
    "                title = dt_tag.get_text(strip=True)\n",
    "                value = dd_tag.get_text(strip=True)\n",
    "                review_data[title] = value\n",
    "\n",
    "        reviews.append(review_data)\n",
    "    return reviews\n",
    "\n",
    "# [5] 상품 + 리뷰 + 설문 크롤링\n",
    "def crawl_products_and_reviews(urls, max_review_count=1000, headless=True):\n",
    "    driver = setup_driver(headless)\n",
    "    product_data, review_data = [], []\n",
    "\n",
    "    for idx, url in enumerate(urls, 1):\n",
    "        print(f\"[{idx}/{len(urls)}] 크롤링 중: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        prod_info = {}\n",
    "        if click_tab(driver, \"구매정보\"):\n",
    "            prod_info = parse_product_detail(driver.page_source)\n",
    "\n",
    "        if click_tab(driver, \"리뷰\"):\n",
    "            time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # 피부타입 분포 (발색력, 지속력 등)\n",
    "            for dl in soup.select(\"dl.poll_type2.type3\"):\n",
    "                dt = dl.select_one(\"dt span\")\n",
    "                dd = dl.select_one(\"dd\")\n",
    "                if dt and dd:\n",
    "                    title = dt.get_text(strip=True)\n",
    "                    dist = {}\n",
    "                    for li in dd.select(\"li\"):\n",
    "                        label = li.select_one(\"span\")\n",
    "                        percent = li.select_one(\"em\")\n",
    "                        if label and percent:\n",
    "                            dist[label.get_text(strip=True)] = percent.get_text(strip=True)\n",
    "                    if dist:\n",
    "                        prod_info[title] = json.dumps({title: dist}, ensure_ascii=False)\n",
    "\n",
    "            total_reviews, current_page = 0, 1\n",
    "            while total_reviews < max_review_count:\n",
    "                time.sleep(1)\n",
    "                reviews = parse_reviews(driver.page_source, max_count=10)\n",
    "                if not reviews:\n",
    "                    break\n",
    "                for rv in reviews:\n",
    "                    review_data.append({\"상품명\": prod_info.get(\"상품명\", \"\"), \"리뷰순번\": total_reviews + 1, **rv})\n",
    "                    total_reviews += 1\n",
    "                    if total_reviews >= max_review_count:\n",
    "                        break\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, f'div.pageing a[data-page-no=\"{current_page + 1}\"]')\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    current_page += 1\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        product_data.append(prod_info)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    product_df = pd.DataFrame(product_data)\n",
    "    review_df = pd.DataFrame(review_data)\n",
    "\n",
    "    product_df[\"상세미디어목록\"] = product_df[\"상세미디어목록\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, list) else \"[]\")\n",
    "    product_df[\"옵션리스트\"] = product_df[\"옵션리스트\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if x else \"[]\")\n",
    "    review_df[\"태그\"] = review_df[\"태그\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "    review_df[\"사용자 피부 정보\"] = review_df[\"사용자 피부 정보\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    return product_df, review_df\n",
    "\n",
    "# [6] 실행\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/선크림 카테고리\"\n",
    "    save_dir = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/선크림\"\n",
    "    csv_files = sorted(glob.glob(os.path.join(folder_path, \"*_상품URL목록.csv\")))\n",
    "\n",
    "    for csv_path in csv_files:\n",
    "        try:\n",
    "            category_name = os.path.basename(csv_path).split(\"_상품URL\")[0]\n",
    "            print(f\"\\n▶ 크롤링 시작: {category_name}\")\n",
    "            df_urls = pd.read_csv(csv_path)\n",
    "            urls = df_urls[\"url\"].dropna().unique().tolist()[:2]\n",
    "            product_df, review_df = crawl_products_and_reviews(urls, max_review_count=5, headless=True)\n",
    "\n",
    "            product_df.to_csv(f\"{save_dir}/{category_name}_상품정보.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            review_df.to_csv(f\"{save_dir}/{category_name}_리뷰정보.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"완료: {category_name} | 상품 {len(product_df)}개, 리뷰 {len(review_df)}개\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"에러 발생 ({csv_path}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a379342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ 카테고리 시작: 메이크업_립메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000209460\n",
      "에러 발생 (/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업 카테고리/메이크업_립메이크업_상품URL목록.csv): no such group\n",
      "\n",
      "▶ 카테고리 시작: 메이크업_베이스메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000208681\n",
      "에러 발생 (/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업 카테고리/메이크업_베이스메이크업_상품URL목록.csv): no such group\n",
      "\n",
      "▶ 카테고리 시작: 메이크업_아이메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000142375\n",
      "에러 발생 (/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업 카테고리/메이크업_아이메이크업_상품URL목록.csv): no such group\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "folder_path = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업 카테고리\"\n",
    "save_dir = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업\"\n",
    "#os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# [0] 드라이버 세팅\n",
    "def setup_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "# [1] 탭 클릭\n",
    "def click_tab(driver, tab_text):\n",
    "    try:\n",
    "        a = driver.find_element(By.XPATH, f\"//ul[contains(@class,'prd_detail_tab')]//a[text()='{tab_text}']\")\n",
    "        a.click()\n",
    "        time.sleep(1)\n",
    "        return True\n",
    "    except:\n",
    "        for a in driver.find_elements(By.CSS_SELECTOR, \".prd_detail_tab a\"):\n",
    "            if tab_text in a.text:\n",
    "                a.click()\n",
    "                time.sleep(1)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# [2] 미디어 파싱\n",
    "def parse_product_media(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    media_urls = []\n",
    "    for tag in soup.select(\"div.detail_area *\"):\n",
    "        src = tag.get(\"src\") or tag.get(\"srcset\")\n",
    "        if src and src.startswith(\"https://\"):\n",
    "            media_urls.append(src)\n",
    "    return media_urls\n",
    "\n",
    "# [3] 제품 상세 파싱\n",
    "def parse_product_detail(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    name = soup.select_one(\"p.prd_name\")\n",
    "    brand = soup.select_one(\"p.prd_brand a\")\n",
    "    old_price = soup.select_one(\"span.price-1 strike\")\n",
    "    sale_price = soup.select_one(\"span.price-2 strong\")\n",
    "    img = soup.select_one(\"#mainImg\")\n",
    "    ing, origin = \"\", \"\"\n",
    "\n",
    "    artc = soup.find(\"div\", id=\"artcInfo\")\n",
    "    if artc:\n",
    "        for dl in artc.select(\"dl.detail_info_list\"):\n",
    "            dt, dd = dl.find(\"dt\"), dl.find(\"dd\")\n",
    "            if dt and dd:\n",
    "                title = dt.get_text(strip=True)\n",
    "                value = dd.get_text(\" \", strip=True)\n",
    "                if \"모든 성분\" in title:\n",
    "                    ing = value\n",
    "                elif \"제조국\" in title:\n",
    "                    origin = value\n",
    "\n",
    "    options = []\n",
    "    for li in soup.select(\"ul.sel_option_list > li[optgoodsinfo]\"):\n",
    "        txt = li.select_one(\"span.txt\").get_text(strip=True)\n",
    "        code, no = li[\"optgoodsinfo\"].split(\":\")\n",
    "        lgc = li.find(\"input\", {\"name\": \"gdasLgcGoodsNo\"})[\"value\"]\n",
    "        options.append({\"옵션명\": txt, \"상품코드\": code, \"아이템번호\": no, \"lgcGoodsNo\": lgc})\n",
    "\n",
    "    media_list = parse_product_media(html)\n",
    "\n",
    "    return {\n",
    "        \"상품명\": name.get_text(strip=True) if name else \"\",\n",
    "        \"브랜드\": brand.get_text(strip=True) if brand else \"\",\n",
    "        \"정가\": old_price.get_text(strip=True) if old_price else \"\",\n",
    "        \"할인가\": sale_price.get_text(strip=True) if sale_price else \"\",\n",
    "        \"이미지\": img[\"src\"] if img else \"\",\n",
    "        \"성분\": ing, \"제조국\": origin,\n",
    "        \"옵션개수\": len(options),\n",
    "        \"옵션리스트\": options,\n",
    "        \"상세미디어목록\": media_list\n",
    "    }\n",
    "\n",
    "# [4] 리뷰 파싱\n",
    "def parse_reviews(html, max_count=10):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    reviews = []\n",
    "    poll_data = {}\n",
    "\n",
    "    ul = soup.find(\"ul\", id=\"gdasList\")\n",
    "    if not ul:\n",
    "        return reviews\n",
    "\n",
    "    for li in ul.find_all(\"li\", recursive=False)[:max_count]:\n",
    "        reviewer = li.select_one(\"p.info_user a.id\")\n",
    "\n",
    "        rating_tag = li.select_one(\"span.point\")\n",
    "        rating, max_score = \"\", \"5.0\"\n",
    "\n",
    "        if rating_tag and 'style' in rating_tag.attrs:\n",
    "            match = re.search(r'width:(\\d+)%', rating_tag['style'])\n",
    "            if match:\n",
    "                percent = int(match.group(1))\n",
    "                rating = str(round((percent / 100) * float(max_score), 1))\n",
    "\n",
    "        #text = rating_tag.get_text(strip=True) if rating_tag else \"\"\n",
    "        #match = re.search(r\"(\\d+)점만점에\\s*(\\d+)점\", text)\n",
    "        #max_score = match.group(1) if match else \"\"\n",
    "\n",
    "        rating = match.group(2) if match else \"\"\n",
    "        score_span = li.select_one(\"div.score_area span.point\")\n",
    "        date = li.select_one(\"span.date\")\n",
    "        opt = li.select_one(\"p.item_option\")\n",
    "        txt = li.select_one(\"div.txt_inner\")\n",
    "        rec = li.select_one(\".recom_area span.num\")\n",
    "\n",
    "\n",
    "        review_data = {\n",
    "            \"리뷰어\": reviewer.get_text(strip=True) if reviewer else \"\",\n",
    "            \"평점\": rating,\n",
    "            \"최대평점\": max_score,\n",
    "            \"날짜\": date.get_text(strip=True) if date else \"\",\n",
    "            \"옵션\": opt.get_text(strip=True).replace(\"[옵션]\", \"\") if opt else \"\",\n",
    "            \"본문\": txt.get_text(\" \", strip=True) if txt else \"\",\n",
    "            \"추천수\": rec.get_text(strip=True) if rec else \"0\",\n",
    "            \"태그\": [s.get_text(strip=True) for s in li.select(\".review_tag span\")],\n",
    "            \"사용자 피부 정보\": [s.get_text(strip=True) for s in li.select(\"p.tag span\")]\n",
    "        }\n",
    "\n",
    "        # [설문 블록 정보 추출] (예: 발색력 아주 만족해요 등)\n",
    "        poll_dl_tags = li.select(\"div.poll_sample dl.poll_type1\")\n",
    "        for dl in poll_dl_tags:\n",
    "            dt_tag = dl.select_one(\"dt span\")\n",
    "            dd_tag = dl.select_one(\"dd span\")\n",
    "            if dt_tag and dd_tag:\n",
    "                title = dt_tag.get_text(strip=True)\n",
    "                value = dd_tag.get_text(strip=True)\n",
    "                review_data[title] = value\n",
    "\n",
    "        reviews.append(review_data)\n",
    "    return reviews\n",
    "\n",
    "# [5] 상품 + 리뷰 + 설문 크롤링\n",
    "def crawl_products_and_reviews(urls, max_review_count=1000, headless=True):\n",
    "    driver = setup_driver(headless)\n",
    "    product_data, review_data = [], []\n",
    "\n",
    "    for idx, url in enumerate(urls, 1):\n",
    "        print(f\"[{idx}/{len(urls)}] 크롤링 중: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        prod_info = {}\n",
    "        if click_tab(driver, \"구매정보\"):\n",
    "            prod_info = parse_product_detail(driver.page_source)\n",
    "\n",
    "        if click_tab(driver, \"리뷰\"):\n",
    "            time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # 피부타입 분포 (발색력, 지속력 등)\n",
    "            for dl in soup.select(\"dl.poll_type2.type3\"):\n",
    "                dt = dl.select_one(\"dt span\")\n",
    "                dd = dl.select_one(\"dd\")\n",
    "                if dt and dd:\n",
    "                    title = dt.get_text(strip=True)\n",
    "                    dist = {}\n",
    "                    for li in dd.select(\"li\"):\n",
    "                        label = li.select_one(\"span\")\n",
    "                        percent = li.select_one(\"em\")\n",
    "                        if label and percent:\n",
    "                            dist[label.get_text(strip=True)] = percent.get_text(strip=True)\n",
    "                    if dist:\n",
    "                        prod_info[title] = json.dumps({title: dist}, ensure_ascii=False)\n",
    "\n",
    "            total_reviews, current_page = 0, 1\n",
    "            while total_reviews < max_review_count:\n",
    "                time.sleep(1)\n",
    "                reviews = parse_reviews(driver.page_source, max_count=10)\n",
    "                if not reviews:\n",
    "                    break\n",
    "                for rv in reviews:\n",
    "                    review_data.append({\"상품명\": prod_info.get(\"상품명\", \"\"), \"리뷰순번\": total_reviews + 1, **rv})\n",
    "                    total_reviews += 1\n",
    "                    if total_reviews >= max_review_count:\n",
    "                        break\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, f'div.pageing a[data-page-no=\"{current_page + 1}\"]')\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    current_page += 1\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        product_data.append(prod_info)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    product_df = pd.DataFrame(product_data)\n",
    "    review_df = pd.DataFrame(review_data)\n",
    "\n",
    "    product_df[\"상세미디어목록\"] = product_df[\"상세미디어목록\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, list) else \"[]\")\n",
    "    product_df[\"옵션리스트\"] = product_df[\"옵션리스트\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if x else \"[]\")\n",
    "    review_df[\"태그\"] = review_df[\"태그\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "    review_df[\"사용자 피부 정보\"] = review_df[\"사용자 피부 정보\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    return product_df, review_df\n",
    "\n",
    "# [6] 실행 (에러 발생 시 이어서 수행)\n",
    "csv_files = sorted(glob.glob(os.path.join(folder_path, \"*_상품URL목록.csv\")))\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        category_full = os.path.splitext(os.path.basename(csv_path))[0].replace(\"_상품URL목록\", \"\")\n",
    "        print(f\"\\n▶ 카테고리 시작: {category_full}\")\n",
    "\n",
    "        df_urls = pd.read_csv(csv_path)\n",
    "        urls = df_urls[\"url\"].dropna().unique().tolist()[:1]\n",
    "\n",
    "        url_batch_size = 120\n",
    "        total_batches = len(urls) // url_batch_size + (1 if len(urls) % url_batch_size > 0 else 0)\n",
    "\n",
    "        for i in range(0, len(urls), url_batch_size):\n",
    "            url_batch = urls[i:i + url_batch_size]\n",
    "            url_batch_no = (i // url_batch_size) + 1\n",
    "\n",
    "            # 이미 저장된 파일이 있다면 건너뜀\n",
    "            product_path = f\"{save_dir}/{category_full}_{url_batch_no}_상품정보.csv\"\n",
    "            review_path = f\"{save_dir}/{category_full}_{url_batch_no}_리뷰정보.csv\"\n",
    "            if os.path.exists(product_path) and os.path.exists(review_path):\n",
    "                print(f\"  이미 완료된 Batch {url_batch_no}, 건너뜀.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  └─ URL Batch {url_batch_no} ({len(url_batch)}개) 크롤링 중...\")\n",
    "            product_df, review_df = crawl_products_and_reviews(url_batch, max_review_count=5, headless=True)\n",
    "\n",
    "            product_df.to_csv(product_path, index=False, encoding=\"utf-8-sig\")\n",
    "            review_df.to_csv(review_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "            print(f\"  저장 완료: {category_full}_{url_batch_no}_상품정보 & 리뷰정보\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생 ({csv_path}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "824a0c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ 카테고리 시작: 메이크업_립메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000209460\n",
      "  저장 완료: 메이크업_립메이크업_1_상품정보 & 리뷰정보\n",
      "\n",
      "▶ 카테고리 시작: 메이크업_베이스메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000208681\n",
      "  저장 완료: 메이크업_베이스메이크업_1_상품정보 & 리뷰정보\n",
      "\n",
      "▶ 카테고리 시작: 메이크업_아이메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000142375\n",
      "  저장 완료: 메이크업_아이메이크업_1_상품정보 & 리뷰정보\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "folder_path = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업 카테고리\"\n",
    "save_dir = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# [0] 드라이버 세팅\n",
    "def setup_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "# [1] 탭 클릭\n",
    "def click_tab(driver, tab_text):\n",
    "    try:\n",
    "        a = driver.find_element(By.XPATH, f\"//ul[contains(@class,'prd_detail_tab')]//a[text()='{tab_text}']\")\n",
    "        a.click()\n",
    "        time.sleep(1)\n",
    "        return True\n",
    "    except:\n",
    "        for a in driver.find_elements(By.CSS_SELECTOR, \".prd_detail_tab a\"):\n",
    "            if tab_text in a.text:\n",
    "                a.click()\n",
    "                time.sleep(1)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# [2] 미디어 파싱\n",
    "def parse_product_media(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    media_urls = []\n",
    "    for tag in soup.select(\"div.detail_area *\"):\n",
    "        src = tag.get(\"src\") or tag.get(\"srcset\")\n",
    "        if src and src.startswith(\"https://\"):\n",
    "            media_urls.append(src)\n",
    "    return media_urls\n",
    "\n",
    "# [3] 제품 상세 파싱\n",
    "def parse_product_detail(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    name = soup.select_one(\"p.prd_name\")\n",
    "    brand = soup.select_one(\"p.prd_brand a\")\n",
    "    old_price = soup.select_one(\"span.price-1 strike\")\n",
    "    sale_price = soup.select_one(\"span.price-2 strong\")\n",
    "    img = soup.select_one(\"#mainImg\")\n",
    "    ing, origin = \"\", \"\"\n",
    "\n",
    "    artc = soup.find(\"div\", id=\"artcInfo\")\n",
    "    if artc:\n",
    "        for dl in artc.select(\"dl.detail_info_list\"):\n",
    "            dt, dd = dl.find(\"dt\"), dl.find(\"dd\")\n",
    "            if dt and dd:\n",
    "                title = dt.get_text(strip=True)\n",
    "                value = dd.get_text(\" \", strip=True)\n",
    "                if \"모든 성분\" in title:\n",
    "                    ing = value\n",
    "                elif \"제조국\" in title:\n",
    "                    origin = value\n",
    "\n",
    "    options = []\n",
    "    for li in soup.select(\"ul.sel_option_list > li[optgoodsinfo]\"):\n",
    "        txt = li.select_one(\"span.txt\").get_text(strip=True)\n",
    "        code, no = li[\"optgoodsinfo\"].split(\":\")\n",
    "        lgc = li.find(\"input\", {\"name\": \"gdasLgcGoodsNo\"})[\"value\"]\n",
    "        options.append({\"옵션명\": txt, \"상품코드\": code, \"아이템번호\": no, \"lgcGoodsNo\": lgc})\n",
    "\n",
    "    media_list = parse_product_media(html)\n",
    "\n",
    "    return {\n",
    "        \"상품명\": name.get_text(strip=True) if name else \"\",\n",
    "        \"브랜드\": brand.get_text(strip=True) if brand else \"\",\n",
    "        \"정가\": old_price.get_text(strip=True) if old_price else \"\",\n",
    "        \"할인가\": sale_price.get_text(strip=True) if sale_price else \"\",\n",
    "        \"이미지\": img[\"src\"] if img else \"\",\n",
    "        \"성분\": ing, \"제조국\": origin,\n",
    "        \"옵션개수\": len(options),\n",
    "        \"옵션리스트\": options,\n",
    "        \"상세미디어목록\": media_list\n",
    "    }\n",
    "\n",
    "# [4] 리뷰 파싱\n",
    "def parse_reviews(html, max_count=10):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    reviews = []\n",
    "    poll_data = {}\n",
    "\n",
    "    ul = soup.find(\"ul\", id=\"gdasList\")\n",
    "    if not ul:\n",
    "        return reviews\n",
    "\n",
    "    for li in ul.find_all(\"li\", recursive=False)[:max_count]:\n",
    "        reviewer = li.select_one(\"p.info_user a.id\")\n",
    "        rating = li.select_one(\"span.point\")\n",
    "        score_span = li.select_one(\"div.score_area span.point\")\n",
    "        date = li.select_one(\"span.date\")\n",
    "        opt = li.select_one(\"p.item_option\")\n",
    "        txt = li.select_one(\"div.txt_inner\")\n",
    "        rec = li.select_one(\".recom_area span.num\")\n",
    "\n",
    "        review_data = {\n",
    "            \"리뷰어\": reviewer.get_text(strip=True) if reviewer else \"\",\n",
    "            \"평점\": re.search(r\"([\\d\\.]+)점\", rating.get_text(strip=True)).group(1) if rating else \"\",\n",
    "            \"최대평점\": re.search(r\"(\\d+)점만점에\", score_span.get_text(strip=True)).group(1) if score_span else \"\",\n",
    "            \"날짜\": date.get_text(strip=True) if date else \"\",\n",
    "            \"옵션\": opt.get_text(strip=True).replace(\"[옵션]\", \"\") if opt else \"\",\n",
    "            \"본문\": txt.get_text(\" \", strip=True) if txt else \"\",\n",
    "            \"추천수\": rec.get_text(strip=True) if rec else \"0\",\n",
    "            \"태그\": [s.get_text(strip=True) for s in li.select(\".review_tag span\")],\n",
    "            \"사용자 피부 정보\": [s.get_text(strip=True) for s in li.select(\"p.tag span\")]\n",
    "        }\n",
    "\n",
    "        # [설문 블록 정보 추출] (예: 발색력 아주 만족해요 등)\n",
    "        poll_dl_tags = li.select(\"div.poll_sample dl.poll_type1\")\n",
    "        for dl in poll_dl_tags:\n",
    "            dt_tag = dl.select_one(\"dt span\")\n",
    "            dd_tag = dl.select_one(\"dd span\")\n",
    "            if dt_tag and dd_tag:\n",
    "                title = dt_tag.get_text(strip=True)\n",
    "                value = dd_tag.get_text(strip=True)\n",
    "                review_data[title] = value\n",
    "\n",
    "        reviews.append(review_data)\n",
    "    return reviews\n",
    "\n",
    "# [5] 상품 + 리뷰 + 설문 크롤링\n",
    "def crawl_products_and_reviews(urls, max_review_count=1000, headless=True):\n",
    "    driver = setup_driver(headless)\n",
    "    product_data, review_data = [], []\n",
    "\n",
    "    for idx, url in enumerate(urls, 1):\n",
    "        print(f\"[{idx}/{len(urls)}] 크롤링 중: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        prod_info = {}\n",
    "        if click_tab(driver, \"구매정보\"):\n",
    "            prod_info = parse_product_detail(driver.page_source)\n",
    "\n",
    "        if click_tab(driver, \"리뷰\"):\n",
    "            time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # 피부타입 분포 (발색력, 지속력 등)\n",
    "            for dl in soup.select(\"dl.poll_type2.type3\"):\n",
    "                dt = dl.select_one(\"dt span\")\n",
    "                dd = dl.select_one(\"dd\")\n",
    "                if dt and dd:\n",
    "                    title = dt.get_text(strip=True)\n",
    "                    dist = {}\n",
    "                    for li in dd.select(\"li\"):\n",
    "                        label = li.select_one(\"span\")\n",
    "                        percent = li.select_one(\"em\")\n",
    "                        if label and percent:\n",
    "                            dist[label.get_text(strip=True)] = percent.get_text(strip=True)\n",
    "                    if dist:\n",
    "                        prod_info[title] = json.dumps({title: dist}, ensure_ascii=False)\n",
    "\n",
    "            total_reviews, current_page = 0, 1\n",
    "            while total_reviews < max_review_count:\n",
    "                time.sleep(1)\n",
    "                reviews = parse_reviews(driver.page_source, max_count=10)\n",
    "                if not reviews:\n",
    "                    break\n",
    "                for rv in reviews:\n",
    "                    review_data.append({\"상품명\": prod_info.get(\"상품명\", \"\"), \"리뷰순번\": total_reviews + 1, **rv})\n",
    "                    total_reviews += 1\n",
    "                    if total_reviews >= max_review_count:\n",
    "                        break\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, f'div.pageing a[data-page-no=\"{current_page + 1}\"]')\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    current_page += 1\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        product_data.append(prod_info)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    product_df = pd.DataFrame(product_data)\n",
    "    review_df = pd.DataFrame(review_data)\n",
    "\n",
    "    product_df[\"상세미디어목록\"] = product_df[\"상세미디어목록\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, list) else \"[]\")\n",
    "    product_df[\"옵션리스트\"] = product_df[\"옵션리스트\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if x else \"[]\")\n",
    "    review_df[\"태그\"] = review_df[\"태그\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "    review_df[\"사용자 피부 정보\"] = review_df[\"사용자 피부 정보\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    return product_df, review_df\n",
    "\n",
    "# [6] 실행 (에러 발생 시 이어서 수행)\n",
    "csv_files = sorted(glob.glob(os.path.join(folder_path, \"*_상품URL목록.csv\")))\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        category_full = os.path.splitext(os.path.basename(csv_path))[0].replace(\"_상품URL목록\", \"\")\n",
    "        print(f\"\\n▶ 카테고리 시작: {category_full}\")\n",
    "\n",
    "        df_urls = pd.read_csv(csv_path)\n",
    "        urls = df_urls[\"url\"].dropna().unique().tolist()[:1]\n",
    "\n",
    "        url_batch_size = 120\n",
    "        total_batches = len(urls) // url_batch_size + (1 if len(urls) % url_batch_size > 0 else 0)\n",
    "\n",
    "        for i in range(0, len(urls), url_batch_size):\n",
    "            url_batch = urls[i:i + url_batch_size]\n",
    "            url_batch_no = (i // url_batch_size) + 1\n",
    "\n",
    "            # 이미 저장된 파일이 있다면 건너뜀\n",
    "            product_path = f\"{save_dir}/{category_full}_{url_batch_no}_상품정보.csv\"\n",
    "            review_path = f\"{save_dir}/{category_full}_{url_batch_no}_리뷰정보.csv\"\n",
    "            if os.path.exists(product_path) and os.path.exists(review_path):\n",
    "                print(f\"  이미 완료된 Batch {url_batch_no}, 건너뜀.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  └─ URL Batch {url_batch_no} ({len(url_batch)}개) 크롤링 중...\")\n",
    "            product_df, review_df = crawl_products_and_reviews(url_batch, max_review_count=5, headless=True)\n",
    "\n",
    "            product_df.to_csv(product_path, index=False, encoding=\"utf-8-sig\")\n",
    "            review_df.to_csv(review_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "            print(f\"  저장 완료: {category_full}_{url_batch_no}_상품정보 & 리뷰정보\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생 ({csv_path}): {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65650f52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bced04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ 카테고리 시작: 메이크업_립메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000209460\n",
      "  저장 완료: 메이크업_립메이크업_1_상품정보 & 리뷰정보\n",
      "\n",
      "▶ 카테고리 시작: 메이크업_베이스메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000208681\n",
      "  저장 완료: 메이크업_베이스메이크업_1_상품정보 & 리뷰정보\n",
      "\n",
      "▶ 카테고리 시작: 메이크업_아이메이크업\n",
      "  └─ URL Batch 1 (1개) 크롤링 중...\n",
      "[1/1] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000142375\n",
      "  저장 완료: 메이크업_아이메이크업_1_상품정보 & 리뷰정보\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "folder_path = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업 카테고리\"\n",
    "save_dir = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# [0] 드라이버 세팅\n",
    "def setup_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "# [1] 탭 클릭\n",
    "def click_tab(driver, tab_text):\n",
    "    try:\n",
    "        a = driver.find_element(By.XPATH, f\"//ul[contains(@class,'prd_detail_tab')]//a[text()='{tab_text}']\")\n",
    "        a.click()\n",
    "        time.sleep(1)\n",
    "        return True\n",
    "    except:\n",
    "        for a in driver.find_elements(By.CSS_SELECTOR, \".prd_detail_tab a\"):\n",
    "            if tab_text in a.text:\n",
    "                a.click()\n",
    "                time.sleep(1)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# [2] 미디어 파싱\n",
    "def parse_product_media(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    media_urls = []\n",
    "    for tag in soup.select(\"div.detail_area *\"):\n",
    "        src = tag.get(\"src\") or tag.get(\"srcset\")\n",
    "        if src and src.startswith(\"https://\"):\n",
    "            media_urls.append(src)\n",
    "    return media_urls\n",
    "\n",
    "# [3] 제품 상세 파싱\n",
    "def parse_product_detail(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    name = soup.select_one(\"p.prd_name\")\n",
    "    brand = soup.select_one(\"p.prd_brand a\")\n",
    "    old_price = soup.select_one(\"span.price-1 strike\")\n",
    "    sale_price = soup.select_one(\"span.price-2 strong\")\n",
    "    img = soup.select_one(\"#mainImg\")\n",
    "    ing, origin = \"\", \"\"\n",
    "\n",
    "    artc = soup.find(\"div\", id=\"artcInfo\")\n",
    "    if artc:\n",
    "        for dl in artc.select(\"dl.detail_info_list\"):\n",
    "            dt, dd = dl.find(\"dt\"), dl.find(\"dd\")\n",
    "            if dt and dd:\n",
    "                title = dt.get_text(strip=True)\n",
    "                value = dd.get_text(\" \", strip=True)\n",
    "                if \"모든 성분\" in title:\n",
    "                    ing = value\n",
    "                elif \"제조국\" in title:\n",
    "                    origin = value\n",
    "\n",
    "    options = []\n",
    "    for li in soup.select(\"ul.sel_option_list > li[optgoodsinfo]\"):\n",
    "        txt = li.select_one(\"span.txt\").get_text(strip=True)\n",
    "        code, no = li[\"optgoodsinfo\"].split(\":\")\n",
    "        lgc = li.find(\"input\", {\"name\": \"gdasLgcGoodsNo\"})[\"value\"]\n",
    "        options.append({\"옵션명\": txt, \"상품코드\": code, \"아이템번호\": no, \"lgcGoodsNo\": lgc})\n",
    "\n",
    "    media_list = parse_product_media(html)\n",
    "\n",
    "    return {\n",
    "        \"상품명\": name.get_text(strip=True) if name else \"\",\n",
    "        \"브랜드\": brand.get_text(strip=True) if brand else \"\",\n",
    "        \"정가\": old_price.get_text(strip=True) if old_price else \"\",\n",
    "        \"할인가\": sale_price.get_text(strip=True) if sale_price else \"\",\n",
    "        \"이미지\": img[\"src\"] if img else \"\",\n",
    "        \"성분\": ing, \"제조국\": origin,\n",
    "        \"옵션개수\": len(options),\n",
    "        \"옵션리스트\": options,\n",
    "        \"상세미디어목록\": media_list\n",
    "    }\n",
    "\n",
    "# [4] 리뷰 파싱\n",
    "def parse_reviews(html, max_count=10):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    reviews = []\n",
    "    poll_data = {}\n",
    "\n",
    "    ul = soup.find(\"ul\", id=\"gdasList\")\n",
    "    if not ul:\n",
    "        return reviews\n",
    "\n",
    "    for li in ul.find_all(\"li\", recursive=False)[:max_count]:\n",
    "        reviewer = li.select_one(\"p.info_user a.id\")\n",
    "        rating = li.select_one(\"span.point\")\n",
    "        score_span = li.select_one(\"div.score_area span.point\")\n",
    "        date = li.select_one(\"span.date\")\n",
    "        opt = li.select_one(\"p.item_option\")\n",
    "        txt = li.select_one(\"div.txt_inner\")\n",
    "        rec = li.select_one(\".recom_area span.num\")\n",
    "\n",
    "        review_data = {\n",
    "            \"리뷰어\": reviewer.get_text(strip=True) if reviewer else \"\",\n",
    "            \"평점\": re.search(r\"([\\d\\.]+)점\", rating.get_text(strip=True)).group(1) if rating else \"\",\n",
    "            \"최대평점\": re.search(r\"(\\d+)점만점에\", score_span.get_text(strip=True)).group(1) if score_span else \"\",\n",
    "            \"날짜\": date.get_text(strip=True) if date else \"\",\n",
    "            \"옵션\": opt.get_text(strip=True).replace(\"[옵션]\", \"\") if opt else \"\",\n",
    "            \"본문\": txt.get_text(\" \", strip=True) if txt else \"\",\n",
    "            \"추천수\": rec.get_text(strip=True) if rec else \"0\",\n",
    "            \"태그\": [s.get_text(strip=True) for s in li.select(\".review_tag span\")],\n",
    "            \"사용자 피부 정보\": [s.get_text(strip=True) for s in li.select(\"p.tag span\")]\n",
    "        }\n",
    "\n",
    "        # [설문 블록 정보 추출] (예: 발색력 아주 만족해요 등)\n",
    "        poll_dl_tags = li.select(\"div.poll_sample dl.poll_type1\")\n",
    "        for dl in poll_dl_tags:\n",
    "            dt_tag = dl.select_one(\"dt span\")\n",
    "            dd_tag = dl.select_one(\"dd span\")\n",
    "            if dt_tag and dd_tag:\n",
    "                title = dt_tag.get_text(strip=True)\n",
    "                value = dd_tag.get_text(strip=True)\n",
    "                review_data[title] = value\n",
    "\n",
    "        reviews.append(review_data)\n",
    "    return reviews\n",
    "\n",
    "# [5] 상품 + 리뷰 + 설문 크롤링\n",
    "def crawl_products_and_reviews(urls, max_review_count=1000, headless=True):\n",
    "    driver = setup_driver(headless)\n",
    "    product_data, review_data = [], []\n",
    "\n",
    "    for idx, url in enumerate(urls, 1):\n",
    "        print(f\"[{idx}/{len(urls)}] 크롤링 중: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        prod_info = {}\n",
    "        if click_tab(driver, \"구매정보\"):\n",
    "            prod_info = parse_product_detail(driver.page_source)\n",
    "\n",
    "        if click_tab(driver, \"리뷰\"):\n",
    "            time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # ============================================# \n",
    "            # 피부타입 분포 (발색력, 지속력 등)\n",
    "            # [✔] 상단 요약 설문 블록 추출 (예: 발색력, 가루날림 등)\n",
    "            poll_summary_div = soup.select_one(\"div.poll_all.clrfix\")\n",
    "            if poll_summary_div:\n",
    "                for dl in poll_summary_div.select(\"dl.poll_type2\"):\n",
    "                    dt_tag = dl.select_one(\"dt span\")\n",
    "                    dd_tag = dl.select_one(\"dd\")\n",
    "                    if dt_tag and dd_tag:\n",
    "                        title = dt_tag.get_text(strip=True)\n",
    "                        dist = {}\n",
    "                        for li in dd_tag.select(\"li\"):\n",
    "                            label = li.select_one(\"span\")\n",
    "                            percent = li.select_one(\"em\")\n",
    "                            if label and percent:\n",
    "                                dist[label.get_text(strip=True)] = percent.get_text(strip=True)\n",
    "                        if dist:\n",
    "                            prod_info[title] = json.dumps({title: dist}, ensure_ascii=False)\n",
    "\n",
    "            total_reviews, current_page = 0, 1\n",
    "            while total_reviews < max_review_count:\n",
    "                time.sleep(1)\n",
    "                reviews = parse_reviews(driver.page_source, max_count=10)\n",
    "                if not reviews:\n",
    "                    break\n",
    "                for rv in reviews:\n",
    "                    review_data.append({\"상품명\": prod_info.get(\"상품명\", \"\"), \"리뷰순번\": total_reviews + 1, **rv})\n",
    "                    total_reviews += 1\n",
    "                    if total_reviews >= max_review_count:\n",
    "                        break\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, f'div.pageing a[data-page-no=\"{current_page + 1}\"]')\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    current_page += 1\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        product_data.append(prod_info)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    product_df = pd.DataFrame(product_data)\n",
    "    review_df = pd.DataFrame(review_data)\n",
    "\n",
    "    product_df[\"상세미디어목록\"] = product_df[\"상세미디어목록\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, list) else \"[]\")\n",
    "    product_df[\"옵션리스트\"] = product_df[\"옵션리스트\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if x else \"[]\")\n",
    "    review_df[\"태그\"] = review_df[\"태그\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "    review_df[\"사용자 피부 정보\"] = review_df[\"사용자 피부 정보\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    return product_df, review_df\n",
    "\n",
    "# [6] 실행 (에러 발생 시 이어서 수행)\n",
    "csv_files = sorted(glob.glob(os.path.join(folder_path, \"*_상품URL목록.csv\")))\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        category_full = os.path.splitext(os.path.basename(csv_path))[0].replace(\"_상품URL목록\", \"\")\n",
    "        print(f\"\\n▶ 카테고리 시작: {category_full}\")\n",
    "\n",
    "        df_urls = pd.read_csv(csv_path)\n",
    "        urls = df_urls[\"url\"].dropna().unique().tolist()[:1]\n",
    "\n",
    "        url_batch_size = 120\n",
    "        total_batches = len(urls) // url_batch_size + (1 if len(urls) % url_batch_size > 0 else 0)\n",
    "\n",
    "        for i in range(0, len(urls), url_batch_size):\n",
    "            url_batch = urls[i:i + url_batch_size]\n",
    "            url_batch_no = (i // url_batch_size) + 1\n",
    "\n",
    "            # 이미 저장된 파일이 있다면 건너뜀\n",
    "            product_path = f\"{save_dir}/{category_full}_{url_batch_no}_상품정보.csv\"\n",
    "            review_path = f\"{save_dir}/{category_full}_{url_batch_no}_리뷰정보.csv\"\n",
    "            if os.path.exists(product_path) and os.path.exists(review_path):\n",
    "                print(f\"  이미 완료된 Batch {url_batch_no}, 건너뜀.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  └─ URL Batch {url_batch_no} ({len(url_batch)}개) 크롤링 중...\")\n",
    "            product_df, review_df = crawl_products_and_reviews(url_batch, max_review_count=5, headless=True)\n",
    "\n",
    "            product_df.to_csv(product_path, index=False, encoding=\"utf-8-sig\")\n",
    "            review_df.to_csv(review_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "            print(f\"  저장 완료: {category_full}_{url_batch_no}_상품정보 & 리뷰정보\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생 ({csv_path}): {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a375db8",
   "metadata": {},
   "source": [
    "# 별점 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c650fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ 카테고리 시작: 메이크업_립메이크업\n",
      "  └─ URL Batch 1 (3개) 크롤링 중...\n",
      "[1/3] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000209460\n",
      "에러 발생 (/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업 카테고리/메이크업_립메이크업_상품URL목록.csv): ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "\n",
      "▶ 카테고리 시작: 메이크업_베이스메이크업\n",
      "  └─ URL Batch 1 (3개) 크롤링 중...\n",
      "[1/3] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000208681\n",
      "[2/3] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000162000\n",
      "[3/3] 크롤링 중: https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000186913\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 248\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  └─ URL Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_batch_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(url_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m개) 크롤링 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 248\u001b[0m product_df, review_df \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_products_and_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_review_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m product_df\u001b[38;5;241m.\u001b[39mto_csv(product_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m review_df\u001b[38;5;241m.\u001b[39mto_csv(review_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 159\u001b[0m, in \u001b[0;36mcrawl_products_and_reviews\u001b[0;34m(urls, max_review_count, headless)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(urls, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] 크롤링 중: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    162\u001b[0m     prod_info \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:454\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:427\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    425\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    136\u001b[0m         method,\n\u001b[1;32m    137\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/urllib3/_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kite/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "folder_path = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업 카테고리\"\n",
    "save_dir = \"/Users/dayeon/dayeoncode/kite/한경토스_강의자료/최종프로젝트/메이크업\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# [0] 드라이버 세팅\n",
    "def setup_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "# [1] 탭 클릭\n",
    "def click_tab(driver, tab_text):\n",
    "    try:\n",
    "        a = driver.find_element(By.XPATH, f\"//ul[contains(@class,'prd_detail_tab')]//a[text()='{tab_text}']\")\n",
    "        a.click()\n",
    "        time.sleep(1)\n",
    "        return True\n",
    "    except:\n",
    "        for a in driver.find_elements(By.CSS_SELECTOR, \".prd_detail_tab a\"):\n",
    "            if tab_text in a.text:\n",
    "                a.click()\n",
    "                time.sleep(1)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# [2] 미디어 파싱\n",
    "def parse_product_media(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    media_urls = []\n",
    "    for tag in soup.select(\"div.detail_area *\"):\n",
    "        src = tag.get(\"src\") or tag.get(\"srcset\")\n",
    "        if src and src.startswith(\"https://\"):\n",
    "            media_urls.append(src)\n",
    "    return media_urls\n",
    "\n",
    "# [3] 제품 상세 파싱\n",
    "def parse_product_detail(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    name = soup.select_one(\"p.prd_name\")\n",
    "    brand = soup.select_one(\"p.prd_brand a\")\n",
    "    old_price = soup.select_one(\"span.price-1 strike\")\n",
    "    sale_price = soup.select_one(\"span.price-2 strong\")\n",
    "    img = soup.select_one(\"#mainImg\")\n",
    "    ing, origin = \"\", \"\"\n",
    "\n",
    "    artc = soup.find(\"div\", id=\"artcInfo\")\n",
    "    if artc:\n",
    "        for dl in artc.select(\"dl.detail_info_list\"):\n",
    "            dt, dd = dl.find(\"dt\"), dl.find(\"dd\")\n",
    "            if dt and dd:\n",
    "                title = dt.get_text(strip=True)\n",
    "                value = dd.get_text(\" \", strip=True)\n",
    "                if \"모든 성분\" in title:\n",
    "                    ing = value\n",
    "                elif \"제조국\" in title:\n",
    "                    origin = value\n",
    "\n",
    "    options = []\n",
    "    for li in soup.select(\"ul.sel_option_list > li[optgoodsinfo]\"):\n",
    "        txt = li.select_one(\"span.txt\").get_text(strip=True)\n",
    "        code, no = li[\"optgoodsinfo\"].split(\":\")\n",
    "        lgc = li.find(\"input\", {\"name\": \"gdasLgcGoodsNo\"})[\"value\"]\n",
    "        options.append({\"옵션명\": txt, \"상품코드\": code, \"아이템번호\": no, \"lgcGoodsNo\": lgc})\n",
    "\n",
    "    media_list = parse_product_media(html)\n",
    "\n",
    "    return {\n",
    "        \"상품명\": name.get_text(strip=True) if name else \"\",\n",
    "        \"브랜드\": brand.get_text(strip=True) if brand else \"\",\n",
    "        \"정가\": old_price.get_text(strip=True) if old_price else \"\",\n",
    "        \"할인가\": sale_price.get_text(strip=True) if sale_price else \"\",\n",
    "        \"이미지\": img[\"src\"] if img else \"\",\n",
    "        \"성분\": ing, \"제조국\": origin,\n",
    "        \"옵션개수\": len(options),\n",
    "        \"옵션리스트\": options,\n",
    "        \"상세미디어목록\": media_list\n",
    "    }\n",
    "\n",
    "# [4] 리뷰 파싱\n",
    "def parse_reviews(html, max_count=10):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    reviews = []\n",
    "    poll_data = {}\n",
    "\n",
    "    ul = soup.find(\"ul\", id=\"gdasList\")\n",
    "    if not ul:\n",
    "        return reviews\n",
    "\n",
    "    for li in ul.find_all(\"li\", recursive=False)[:max_count]:\n",
    "        reviewer = li.select_one(\"p.info_user a.id\")\n",
    "        rating = li.select_one(\"span.point\")\n",
    "        score_span = li.select_one(\"div.score_area span.point\")\n",
    "        date = li.select_one(\"span.date\")\n",
    "        opt = li.select_one(\"p.item_option\")\n",
    "        txt = li.select_one(\"div.txt_inner\")\n",
    "        rec = li.select_one(\".recom_area span.num\")\n",
    "\n",
    "        rating_tag = li.select_one(\"span.point\")\n",
    "        rating_text = rating_tag.get_text(strip=True) if rating_tag else \"\"\n",
    "\n",
    "        # 기본값 설정\n",
    "        rating = \"\"\n",
    "        max_score = \"\"\n",
    "\n",
    "        # 평점 텍스트에서 정규식 추출\n",
    "        match = re.search(r\"(\\d+)점만점에\\s*(\\d+)점\", rating_text)\n",
    "        if match:\n",
    "            max_score = match.group(1)\n",
    "            rating = match.group(2)\n",
    "\n",
    "\n",
    "        review_data = {\n",
    "            \"리뷰어\": reviewer.get_text(strip=True) if reviewer else \"\",\n",
    "            \"평점\": rating,\n",
    "            \"최대평점\": max_score,\n",
    "            \"날짜\": date.get_text(strip=True) if date else \"\",\n",
    "            \"옵션\": opt.get_text(strip=True).replace(\"[옵션]\", \"\") if opt else \"\",\n",
    "            \"본문\": txt.get_text(\" \", strip=True) if txt else \"\",\n",
    "            \"추천수\": rec.get_text(strip=True) if rec else \"0\",\n",
    "            \"태그\": [s.get_text(strip=True) for s in li.select(\".review_tag span\")],\n",
    "            \"사용자 피부 정보\": [s.get_text(strip=True) for s in li.select(\"p.tag span\")]\n",
    "        }\n",
    "\n",
    "        # [설문 블록 정보 추출] (예: 발색력 아주 만족해요 등)\n",
    "        poll_dl_tags = li.select(\"div.poll_sample dl.poll_type1\")\n",
    "        for dl in poll_dl_tags:\n",
    "            dt_tag = dl.select_one(\"dt span\")\n",
    "            dd_tag = dl.select_one(\"dd span\")\n",
    "            if dt_tag and dd_tag:\n",
    "                title = dt_tag.get_text(strip=True)\n",
    "                value = dd_tag.get_text(strip=True)\n",
    "                review_data[title] = value\n",
    "\n",
    "        reviews.append(review_data)\n",
    "    return reviews\n",
    "\n",
    "# [5] 상품 + 리뷰 + 설문 크롤링\n",
    "def crawl_products_and_reviews(urls, max_review_count=1000, headless=True):\n",
    "    driver = setup_driver(headless)\n",
    "    product_data, review_data = [], []\n",
    "\n",
    "    for idx, url in enumerate(urls, 1):\n",
    "        print(f\"[{idx}/{len(urls)}] 크롤링 중: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        prod_info = {}\n",
    "        if click_tab(driver, \"구매정보\"):\n",
    "            prod_info = parse_product_detail(driver.page_source)\n",
    "\n",
    "        if click_tab(driver, \"리뷰\"):\n",
    "            time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # ============================================# \n",
    "            # 피부타입 분포 (발색력, 지속력 등)\n",
    "            # [✔] 상단 요약 설문 블록 추출 (예: 발색력, 가루날림 등)\n",
    "            poll_summary_div = soup.select_one(\"div.poll_all.clrfix\")\n",
    "            if poll_summary_div:\n",
    "                for dl in poll_summary_div.select(\"dl.poll_type2\"):\n",
    "                    dt_tag = dl.select_one(\"dt span\")\n",
    "                    dd_tag = dl.select_one(\"dd\")\n",
    "                    if dt_tag and dd_tag:\n",
    "                        title = dt_tag.get_text(strip=True)\n",
    "                        dist = {}\n",
    "                        for li in dd_tag.select(\"li\"):\n",
    "                            label = li.select_one(\"span\")\n",
    "                            percent = li.select_one(\"em\")\n",
    "                            if label and percent:\n",
    "                                dist[label.get_text(strip=True)] = percent.get_text(strip=True)\n",
    "                        if dist:\n",
    "                            prod_info[title] = json.dumps({title: dist}, ensure_ascii=False)\n",
    "\n",
    "            total_reviews, current_page = 0, 1\n",
    "            while total_reviews < max_review_count:\n",
    "                time.sleep(1)\n",
    "                reviews = parse_reviews(driver.page_source, max_count=10)\n",
    "                if not reviews:\n",
    "                    break\n",
    "                for rv in reviews:\n",
    "                    review_data.append({\"상품명\": prod_info.get(\"상품명\", \"\"), \"리뷰순번\": total_reviews + 1, **rv})\n",
    "                    total_reviews += 1\n",
    "                    if total_reviews >= max_review_count:\n",
    "                        break\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, f'div.pageing a[data-page-no=\"{current_page + 1}\"]')\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    current_page += 1\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        product_data.append(prod_info)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    product_df = pd.DataFrame(product_data)\n",
    "    review_df = pd.DataFrame(review_data)\n",
    "\n",
    "    product_df[\"상세미디어목록\"] = product_df[\"상세미디어목록\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, list) else \"[]\")\n",
    "    product_df[\"옵션리스트\"] = product_df[\"옵션리스트\"].apply(lambda x: json.dumps(x, ensure_ascii=False) if x else \"[]\")\n",
    "    review_df[\"태그\"] = review_df[\"태그\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "    review_df[\"사용자 피부 정보\"] = review_df[\"사용자 피부 정보\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    return product_df, review_df\n",
    "\n",
    "# [6] 실행 (에러 발생 시 이어서 수행)\n",
    "csv_files = sorted(glob.glob(os.path.join(folder_path, \"*_상품URL목록.csv\")))\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        category_full = os.path.splitext(os.path.basename(csv_path))[0].replace(\"_상품URL목록\", \"\")\n",
    "        print(f\"\\n▶ 카테고리 시작: {category_full}\")\n",
    "\n",
    "        df_urls = pd.read_csv(csv_path)\n",
    "        urls = df_urls[\"url\"].dropna().unique().tolist()\n",
    "\n",
    "        url_batch_size = 100\n",
    "        total_batches = len(urls) // url_batch_size + (1 if len(urls) % url_batch_size > 0 else 0)\n",
    "\n",
    "        for i in range(0, len(urls), url_batch_size):\n",
    "            url_batch = urls[i:i + url_batch_size]\n",
    "            url_batch_no = (i // url_batch_size) + 1\n",
    "\n",
    "            # 이미 저장된 파일이 있다면 건너뜀\n",
    "            product_path = f\"{save_dir}/{category_full}_{url_batch_no}_상품정보.csv\"\n",
    "            review_path = f\"{save_dir}/{category_full}_{url_batch_no}_리뷰정보.csv\"\n",
    "            if os.path.exists(product_path) and os.path.exists(review_path):\n",
    "                print(f\"  이미 완료된 Batch {url_batch_no}, 건너뜀.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  └─ URL Batch {url_batch_no} ({len(url_batch)}개) 크롤링 중...\")\n",
    "            product_df, review_df = crawl_products_and_reviews(url_batch, max_review_count=5, headless=True)\n",
    "\n",
    "            product_df.to_csv(product_path, index=False, encoding=\"utf-8-sig\")\n",
    "            review_df.to_csv(review_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "            print(f\"  저장 완료: {category_full}_{url_batch_no}_상품정보 & 리뷰정보\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생 ({csv_path}): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ffe32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
